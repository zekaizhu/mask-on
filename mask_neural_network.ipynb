{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2a - Convolutional nets\n",
    "The purpose of this assignment is to help the student get started with convolutional image recogition models for the course project. This assignment will explore how convolutional nets work in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Northwestern MSiA 432 Deep learning \n",
    "    Assignment #2 starter code, Spring 2020.\n",
    "    \n",
    "    This code demonstrates the use of transfer learning to speed up\n",
    "    the training process for a convolutional neural network.\n",
    "    \n",
    "    Notes:\n",
    "        - Heatmaps may appear black in the first few epochs. Wait until accuracy improves.\n",
    "        - The native image size is 224x224 for VGG, resize/crop your images to match\n",
    "        - Filter visualization is slow, change vizfilt_timeout for more speed or accuracy\n",
    "        - Be sure to rename/delete the basepath when changing model parameters, e.g. layers or random labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show devices\n",
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print([x.name for x in local_device_protos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obligatory imports\n",
    "import os, time, numpy as np, imageio, random, pandas as pd, socket, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # Workaround for Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set project configuration\n",
    "basepath      = \"assignment2-data\"\n",
    "imsize        = (32, 32) # n x n square images, VGG default is 224x224. Remember to change this.\n",
    "# imsize = (64, 64)\n",
    "# imsize = (128, 128)\n",
    "tsize         = imsize + (3,)\n",
    "trainfolder   = os.path.join(basepath, 'train')\n",
    "testfolder    = os.path.join(basepath, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings    \n",
    "vggblocks     = 5        # Number of VGG blocks to create, 0-5 blocks\n",
    "# vggblocks = 1\n",
    "# vggblocks = 2\n",
    "# vggblocks = 3\n",
    "# vggblocks = 4\n",
    "xferlearning  = 9       # Enable transfer learning up to layer n (max 12, -1 = off)\n",
    "# xferlearning = 11\n",
    "# xferlearning = 1\n",
    "# xferlearning = 2\n",
    "# xferlearning = 3\n",
    "# xferlearning = 6\n",
    "# xferlearning = 7\n",
    "# xferlearning = 5\n",
    "freeze_conv   = False    # Freeze convolutional layers\n",
    "fclayersize   = 128      # Size of fully connected (FC) layers\n",
    "fclayers      = 1        # Number of FC layers\n",
    "fcdropout     = 0.0      # Dropout regularization factor for FC layers\n",
    "#fcdropout     = 0.2\n",
    "#fcdropout     = 0.3\n",
    "#fcdropout     = 0.4\n",
    "#fcdropout     = 0.5\n",
    "#fcdropout     = 0.6\n",
    "#fcdropout     = 0.7\n",
    "alpha         = 0.0      # Leaky ReLU alpha\n",
    "l1_reg        = 0.0      # L1 regularization for FC\n",
    "l2_reg        = 0.0      # L2 regularization for FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer settings\n",
    "optimizer = Adam()   \n",
    "#optimizer = SGD()   \n",
    "#optimizer = RMSprop()   \n",
    "#optimizer = Adagrad()   \n",
    "batch_size, nb_epoch = 32, 10000 # Change for early stopping regularization\n",
    "batchnorm     = True     # Batch normalization\n",
    "checkpoint    = True     # Checkpoint models to continue training\n",
    "\n",
    "# Visualization settings\n",
    "hsv             = False    # Convert images to Hue/Saturation/Value to be more robust to color variations\n",
    "#hsv             = True\n",
    "# vizfilt_timeout = 3\n",
    "vizfilt_timeout = 5000        # Decrease for speed, increase for better viz. 0 = off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpointing/stats\n",
    "modeltype     = 'vgg'      # Use default VGG model\n",
    "#modeltype     = 'xception' # New Xception model, recommended imsize = (64, 64) or larger\n",
    "modelarch     = '%s%d-fcl%d-fcs%d-%s-%s' % (modeltype, vggblocks, fclayers, fclayersize, 'hsv' if hsv else 'rgb', socket.gethostname())\n",
    "modelid       = os.path.join(basepath, 'model.h5')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG net definition starts here. Change the vggblocks to set how many blocks to transfer\n",
    "def make_vgg():\n",
    "    from keras.models import Model\n",
    "    from keras.regularizers import l1_l2\n",
    "    from keras.layers import Flatten, Dense, Input, Convolution2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "    from keras.layers.advanced_activations import LeakyReLU\n",
    "    \n",
    "    img_input = Input(shape=tsize)\n",
    "    if vggblocks == 0: x = img_input \n",
    "    if vggblocks >= 1: # Block 1\n",
    "        x = LeakyReLU(alpha)(Convolution2D(64, (3, 3), padding='same', name='block1_conv1')(img_input))\n",
    "        x = LeakyReLU(alpha)(Convolution2D(64, (3, 3), padding='same', name='block1_conv2')(x))\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "        if batchnorm: x = BatchNormalization()(x)\n",
    "    if vggblocks >= 2: # Block 2\n",
    "        x = LeakyReLU(alpha)(Convolution2D(128, (3, 3), padding='same', name='block2_conv1')(x))\n",
    "        x = LeakyReLU(alpha)(Convolution2D(128, (3, 3), padding='same', name='block2_conv2')(x))\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "        if batchnorm: x = BatchNormalization()(x)\n",
    "    if vggblocks >= 3: # Block 3\n",
    "        x = LeakyReLU(alpha)(Convolution2D(256, (3, 3), padding='same', name='block3_conv1')(x))\n",
    "        x = LeakyReLU(alpha)(Convolution2D(256, (3, 3), padding='same', name='block3_conv2')(x))\n",
    "        x = LeakyReLU(alpha)(Convolution2D(256, (3, 3), padding='same', name='block3_conv3')(x))\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "        if batchnorm: x = BatchNormalization()(x)\n",
    "    if vggblocks >= 4: # Block 4\n",
    "        x = LeakyReLU(alpha)(Convolution2D(512, (3, 3), padding='same', name='block4_conv1')(x))\n",
    "        x = LeakyReLU(alpha)(Convolution2D(512, (3, 3), padding='same', name='block4_conv2')(x))\n",
    "        x = LeakyReLU(alpha)(Convolution2D(512, (3, 3), padding='same', name='block4_conv3')(x))\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "        if batchnorm: x = BatchNormalization()(x)\n",
    "    if vggblocks >= 5: # Block 5\n",
    "        x = LeakyReLU(alpha)(Convolution2D(512, (3, 3), padding='same', name='block5_conv1')(x))\n",
    "        x = LeakyReLU(alpha)(Convolution2D(512, (3, 3), padding='same', name='block5_conv2')(x))\n",
    "        x = LeakyReLU(alpha)(Convolution2D(512, (3, 3), padding='same', name='block5_conv3')(x))\n",
    "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "        if batchnorm: x = BatchNormalization()(x)\n",
    "        \n",
    "    x = Flatten(name='flatten')(x)\n",
    "    for i in range(fclayers): \n",
    "        x = LeakyReLU(alpha)(Dense(fclayersize, kernel_regularizer=l1_l2(l1_reg, l2_reg))(x))\n",
    "        if fcdropout > 0:\n",
    "            x = Dropout(fcdropout)(x)\n",
    "    x = Dense(len(obj_classes), activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    inputs = img_input\n",
    "    model = Model(inputs, x, name='vgg16')\n",
    "    \n",
    "    # VGG Transfer weights\n",
    "    from keras.applications import vgg16\n",
    "    import keras.layers.convolutional\n",
    "    vgg16model = vgg16.VGG16(include_top=False)\n",
    "    modelconv = [l for l in model.layers if type(l) == keras.layers.convolutional.Conv2D]\n",
    "    vgg16conv = [l for l in vgg16model.layers if type(l) == keras.layers.convolutional.Conv2D]\n",
    "    \n",
    "    for i, l in enumerate(modelconv):\n",
    "        if i > xferlearning: continue # Transfer only first n layers\n",
    "        print('**** Transferring layer %d: %s from VGG ****' % (i, l))\n",
    "        weights = vgg16conv[i].get_weights()\n",
    "        modelconv[i].set_weights(weights)\n",
    "        if freeze_conv: l.trainable = False\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model, img_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ------ CPU/GPU memory fix -------\n",
    "import tensorflow as tf, keras.backend.tensorflow_backend as ktf\n",
    "def get_session(gpu_fraction=0.45):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction, allow_growth=True)\n",
    "    return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "ktf.set_session(get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create demo data\n",
    "def makedata(basepath):\n",
    "    from keras.datasets import cifar10\n",
    "    from keras.utils.generic_utils import Progbar    \n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    obj_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] \n",
    "    for (X_data, y_data, bp) in [(X_train, y_train, trainfolder), (X_test, y_test, testfolder)]:\n",
    "        if os.path.exists(bp): return\n",
    "        for c in obj_classes: os.makedirs(os.path.join(bp, c), exist_ok=True)\n",
    "        pb = Progbar(len(y_data), interval=1)\n",
    "        print('\\nMaking data folder')\n",
    "        for i, (im, lbl) in enumerate(zip(X_data, y_data)):\n",
    "            pn = os.path.join(bp, obj_classes[int(lbl)], \"%d.png\" % i)\n",
    "            pb.update(i)\n",
    "            if not os.path.exists(pn): imageio.imsave(pn, im)\n",
    "\n",
    "makedata(basepath) # Comment out this line to use your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,                         # rescale data\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,    \n",
    "    \n",
    "    rotation_range=0,                       # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # tested rotation_range = 30/60/90/120/150\n",
    "    #rotation_range=30,\n",
    "    #rotation_range=60,\n",
    "    #rotation_range=90,\n",
    "    #rotation_range=120,\n",
    "    #rotation_range=150,\n",
    "    \n",
    "    width_shift_range=0.0,                  # randomly shift images horizontally (fraction of total width)\n",
    "    # tested width_shift_range = 0.25/0.5/0.75\n",
    "    #width_shift_range=0.25,\n",
    "    #width_shift_range=0.5,\n",
    "    #width_shift_range=0.75,\n",
    "    \n",
    "    height_shift_range=0.0,                 # randomly shift images vertically (fraction of total height)\n",
    "    # tested height_range = 0.25/0.5/0.75\n",
    "    #height_shift_range=0.25,\n",
    "    #height_shift_range=0.5,\n",
    "    #height_shift_range=0.75,\n",
    "    \n",
    "    horizontal_flip=True,                   # randomly flip images\n",
    "    # tested horizontal_flip = False\n",
    "    #horizontal_flip=False,\n",
    "    \n",
    "    vertical_flip=False                     # randomly flip images\n",
    "    # tested vertical_flip = True\n",
    "    #vertical_flip=True\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        trainfolder,\n",
    "        target_size=imsize,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        testfolder,\n",
    "        target_size=imsize,\n",
    "        batch_size=-1)\n",
    "\n",
    "X_test, Y_test = test_generator.next()\n",
    "\n",
    "obj_classes = sorted(train_generator.class_indices.keys())\n",
    "class_to_idx = dict([(y, x) for (x,y) in enumerate(obj_classes)])\n",
    "img_rows, img_cols, img_channels = X_test.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Xception definition\n",
    "def make_xception():\n",
    "    import keras\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "    from keras.regularizers import l1\n",
    "    xmodel = keras.applications.Xception(include_top=False)\n",
    "    x = xmodel.output\n",
    "    #for layer in xmodel.layers: layer.trainable = False\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, kernel_regularizer=l1(1e-7))(x)\n",
    "    predictions = Dense(len(obj_classes), activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=xmodel.input, outputs=predictions)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam())\n",
    "    model.summary()\n",
    "    return model, xmodel.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modeltype == 'vgg':\n",
    "    model, img_input = make_vgg()    \n",
    "elif modeltype == 'xception':\n",
    "    model, img_input = make_xception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Visualization code\n",
    "def viz_losses(stats): \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,6))\n",
    "    epoch = len(stats)\n",
    "    convlayers = len([l.name for l in model.layers if 'conv' in l.name])\n",
    "    blocks     = len(set([l.name.split('_')[0] for l in model.layers if 'block' in l.name]))\n",
    "    dense      = len([l.name for l in model.layers if 'dense' in l.name])\n",
    "    fcsize     = model.layers[-1].input_shape[1]\n",
    "    fig.suptitle(\"Training %s blocks=%d, conv=%d, dense=%d, fcsize=%d, epoch=%d\" % (model.name, blocks, convlayers, dense, fcsize, epoch))\n",
    "    ax1.plot(stats['Train loss'].values, label='Train loss', color='blue')\n",
    "    ax1.plot(stats['Test loss'].values, label='Test loss', color='green')\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.plot(stats['Accuracy'].values, label='Test accuracies', color='red')\n",
    "    ax2.plot(stats['Train accuracy'].values, label='Train accuracies', color='blue')\n",
    "    ax2.axhline(1.0/len(obj_classes), linestyle='dashed', color='gray')\n",
    "    dataset = pd.Series(train_generator.classes)\n",
    "    chance = dataset.value_counts().max() / dataset.value_counts().sum()\n",
    "    ax2.text(0, chance, 'Chance')\n",
    "    ax2.axhline(np.max(stats['Accuracy']), linestyle='dashed', color='red')\n",
    "    ax2.text(0, np.max(stats['Accuracy']), 'Best')    \n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.set_title('Accuracy: %0.2f%%' % (100.0*stats['Accuracy'].values[-1]))     \n",
    "    ax1.legend(), ax2.legend()\n",
    "    plt.savefig(os.path.join(basepath, 'loss-%s.png' % modelarch))\n",
    "    plt.show()    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Explanations\n",
    "import skimage.exposure, skimage.filters\n",
    "from skimage.color import gray2rgb\n",
    "from keras import backend as K\n",
    "\n",
    "def hide_axes(ax): ax.set_xticks([]), ax.set_yticks([])\n",
    "class Heatmap:\n",
    "    def __init__(self, model, obj_classes):\n",
    "        self.obj_classes = obj_classes\n",
    "        self.nclasses    = len(obj_classes)\n",
    "        self.model       = model\n",
    "    \n",
    "    def make_masks(self, im, n=8, maskval=0.1):\n",
    "        masks = []\n",
    "        xwidth, ywidth = int(np.ceil(im.shape[0]/n)), int(np.ceil(im.shape[1]/n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                mask = np.ones(im.shape[:2])\n",
    "                mask[(i*xwidth):((i+1)*xwidth), (j*ywidth):((j+1)*ywidth)] = maskval\n",
    "                mask = skimage.filters.gaussian(mask, 1) # Change this for local mask smoothing\n",
    "                masks.append(mask)\n",
    "        return np.array(masks)\n",
    "\n",
    "    def get_slice_masks(self, im, n_segments=16, blur=0.03):\n",
    "        from skimage.segmentation import slic\n",
    "        segments = slic(im, n_segments=n_segments, sigma=5)\n",
    "        masks = []\n",
    "        # loop over the unique segment values\n",
    "        for (i, segVal) in enumerate(np.unique(segments)):\n",
    "            # construct a mask for the segment\n",
    "            mask = np.zeros(im.shape[:2], dtype=\"float32\")\n",
    "            mask[segments == segVal] = 1\n",
    "            mask = skimage.filters.gaussian(mask, im.shape[1]*blur) # Change this for local mask smoothing            \n",
    "            masks.append(mask)\n",
    "        return np.array(masks), segments\n",
    "        \n",
    "    def explain_prediction_heatmap(self, im, actual):\n",
    "        import skimage.color\n",
    "        def hsv_fn(im): return skimage.color.hsv2rgb(im) if hsv else im\n",
    "        plt.imshow(hsv_fn(im), interpolation='bilinear'), plt.xticks([]), plt.yticks([]), plt.title('Full image'), plt.show(), plt.close()\n",
    "        masks = np.concatenate([self.make_masks(im, n=i) for i in (9, 7, 5, 3, 2)])\n",
    "        #masks, segments = self.get_slice_masks(im)        \n",
    "        masknorm = masks.sum(axis=0)\n",
    "        heatmaps = np.zeros((self.nclasses,) + im.shape[:2])\n",
    "        for m in masks:\n",
    "            prediction = self.model.predict(np.expand_dims(im*gray2rgb(m), 0))\n",
    "            for c in range(self.nclasses):\n",
    "                heatmaps[c] += (prediction[0][c]*m)\n",
    "        for h in heatmaps: h = h / masknorm\n",
    "        fig, axes = plt.subplots(2, self.nclasses + 1, figsize=(20, 5))\n",
    "        #axes[0,0].imshow(hsv(im)), axes[1,0].imshow(mark_boundaries(im, segments))        \n",
    "        axes[0,0].imshow(hsv_fn(im)), axes[1,0].imshow(im)        \n",
    "        \n",
    "        axes[0,0].set_title(actual)\n",
    "        axes[1,0].set_title('HSV' if hsv else 'RGB')        \n",
    "        hide_axes(axes[0,0]), hide_axes(axes[1,0])       \n",
    "        predictions = np.sum(heatmaps, axis=(1,2,))\n",
    "        predictions /= predictions.max()\n",
    "        for n, i in enumerate(np.argsort(predictions)[::-1][:self.nclasses]):\n",
    "            h = ((255 * heatmaps[i])/heatmaps[i].max()).astype('uint16')\n",
    "            try:\n",
    "                h = skimage.exposure.equalize_adapthist(h)\n",
    "            except IndexError: pass\n",
    "            h = skimage.filters.gaussian(h, 1) # Change this for global mask smoothing\n",
    "            axes[0, n+1].imshow(gray2rgb(h))\n",
    "            axes[1, n+1].imshow(gray2rgb(h) * hsv_fn(im) * (0.5 + 0.5*predictions[i]))  \n",
    "            hide_axes(axes[0, n+1]), hide_axes(axes[1, n+1])        \n",
    "            axes[0, n+1].set_title(self.obj_classes[i] + ': %0.1f%%' % (100*predictions[i]/predictions.sum()))\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(os.path.join(basepath, 'heatmap-%05d.png') % np.random.randint(0, 99999))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "def deprocess_image(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x = x*0.1 + 0.5\n",
    "    x = np.clip(x, 0, 1) * 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def viz_filter_max(layer_name, filter_index=0, max_steps=9999, timeout=3):\n",
    "    from keras.utils.generic_utils import Progbar            \n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    grads = K.gradients(loss, img_input)[0]\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "    iterate = K.function([img_input], [loss, grads])\n",
    "    step = 1e-0\n",
    "    input_img_data = np.random.random((1, img_rows, img_cols, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    tm = time.time()\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "        if time.time() - tm > timeout:\n",
    "            plt.text(0.1, 0.1, \"Filter viz timeout: %d\" % timeout, color='red')\n",
    "            break\n",
    "    img = input_img_data[0]\n",
    "    img = deprocess_image(img)\n",
    "    fig = plt.imshow(img)    \n",
    "    hide_axes(fig.axes)\n",
    "    return layer_output\n",
    "    \n",
    "def viz_filters(model, img_input, img_rows, img_cols, nbfilters=3, timeout=5000):\n",
    "    tm = time.time()\n",
    "    print(\"Visualizing filters (CTRL-C to cancel)\")\n",
    "    try:         \n",
    "        for layer_name in sorted(layer_dict.keys()):\n",
    "            if time.time() - tm > timeout:\n",
    "                print(\"Filter visualization timed out: %d. Change timeout in viz_filters().\" % timeout)\n",
    "                break\n",
    "            if not hasattr(layer_dict[layer_name], 'filters'): continue\n",
    "            nfilters = layer_dict[layer_name].filters\n",
    "            fig, ax = plt.subplots(1, nbfilters, figsize=(8, 4))\n",
    "            fig.suptitle(\"Layer %s has %d filters\" % (layer_name, nfilters))            \n",
    "            for j in range(nbfilters):\n",
    "                plt.subplot(1, nbfilters, j + 1)\n",
    "                viz_filter_max(layer_name, random.randint(0, nfilters-1), timeout=vizfilt_timeout)\n",
    "            fig.tight_layout()    \n",
    "            plt.savefig(os.path.join(basepath, 'filters-%s-%s.png' % (modelarch, layer_name))) \n",
    "            plt.show(), plt.close()\n",
    "    except KeyboardInterrupt: return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(im, y):\n",
    "    pred = model.predict(np.expand_dims(im, 0))\n",
    "    cls = np.argmax(y)\n",
    "    heatmap = Heatmap(model, obj_classes)                \n",
    "    heatmap.explain_prediction_heatmap(im, obj_classes[cls])\n",
    "    \n",
    "    print(\"Actual: %s(%d)\" % (obj_classes[cls], cls))\n",
    "    for cls in list(reversed(np.argsort(pred)[0]))[:5]:\n",
    "        conf = float(pred[0, cls])/pred.sum()\n",
    "        print(\"    predicted: %010s(%d), confidence=%0.2f [%-10s]\" % (obj_classes[cls], cls, conf, \"*\" * int(10*conf)))\n",
    "    return pred\n",
    "\n",
    "def confusion_matrix(model, X, T, accpct):\n",
    "    import seaborn\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    Y_pred = model.predict(X)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    y_test = np.argmax(T, axis=1)\n",
    "    print('Confusion Matrix')\n",
    "    data = confusion_matrix(y_test, y_pred)\n",
    "    data = data / data.sum(axis=1)\n",
    "    #print('Classification Report')\n",
    "    #print(classification_report(y_test, y_pred, target_names=obj_classes))\n",
    "    seaborn.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    seaborn.heatmap(data, annot=data*100, fmt='0.0f', cmap='Wistia', xticklabels=obj_classes, yticklabels=obj_classes)\n",
    "    plt.xlabel('Predicted'), plt.ylabel('Actual'), plt.title('Confusion matrix (ACC %0.2f%%)' % (accpct*100))\n",
    "    plt.show(), plt.close()\n",
    "\n",
    "def tsne_viz(model, X, Y, accpct, n=500):\n",
    "    import sklearn.manifold, matplotlib.cm as cm\n",
    "    predictions = model.predict(X)    \n",
    "    colors = iter(cm.rainbow(np.linspace(0, 1, len(obj_classes))))\n",
    "    X_embedded = sklearn.manifold.TSNE(n_components=2).fit_transform(predictions[:n])\n",
    "    for d in range(len(obj_classes)):\n",
    "        xx = X_embedded[Y[:n][:, d] == 1, 0]\n",
    "        yy = X_embedded[Y[:n][:, d] == 1, 1]\n",
    "        plt.scatter(xx, yy, c=[next(colors)], label=obj_classes[d])\n",
    "        t = plt.text(np.median(xx), np.median(yy), obj_classes[d], fontsize=24)\n",
    "        t.set_bbox({'facecolor': 'white', 'alpha': 0.75})\n",
    "    plt.title('T-SNE viz - Accuracy: %0.2f%%' % (accpct*100)), plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint and os.path.exists(modelid): \n",
    "    print(\"**** Loading existing model: %s ****\" % modelid)\n",
    "    try:\n",
    "        model.load_weights(modelid)\n",
    "    except ValueError:\n",
    "        print(\"Model restore failed. Model topology must match to restore weights. Please delete weight checkpoint model.h5.\")\n",
    "    except OSError:\n",
    "        print(\"Model checkpoint corrupted. Please delete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Training code\n",
    "from IPython.display import clear_output\n",
    "\n",
    "tm = time.time()\n",
    "trainstats = pd.DataFrame(columns=('Train loss', 'Test loss', 'Accuracy', 'Train accuracy'))\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "class VizTraining(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        clear_output(wait=True)\n",
    "        tacc = logs.get('val_accuracy')\n",
    "        trainstats.loc[len(trainstats)] = (logs.get('loss'), logs.get('val_loss'), tacc, logs.get('acc')) \n",
    "        confusion_matrix(model, X_test, Y_test, tacc)\n",
    "        tsne_viz(model, X_test, Y_test, tacc)        \n",
    "        viz_losses(trainstats)\n",
    "        t_ind = random.randint(0, len(X_test) - 1)        \n",
    "        test_prediction(X_test[t_ind], Y_test[t_ind])    \n",
    "        if vizfilt_timeout > 0 and np.random.randint(0, 3) == 0: \n",
    "            viz_filters(model, img_input, img_rows, img_cols)\n",
    "        if checkpoint: model.save(modelid, overwrite=True)\n",
    "        print(\"Total training time: %0.2f min, epoch: %d\" % ((time.time() - tm)/60.0, len(trainstats))) \n",
    "        print(\"Average time per epoch: %0.2f s\" % ((time.time() - tm)/len(trainstats)))\n",
    "\n",
    "loss = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=int(len(train_generator.filenames)/batch_size),\n",
    "    validation_data=(X_test, Y_test),\n",
    "    validation_steps=1,\n",
    "    verbose=1, epochs=100,\n",
    "    use_multiprocessing=True,\n",
    "    workers=4,\n",
    "    callbacks=[VizTraining()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
